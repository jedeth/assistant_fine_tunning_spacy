
[paths]
train = "C:\Users\jde-thesut\Documents\Assistant_fine_tunning_Spacy\train.spacy"
dev = "C:\Users\jde-thesut\Documents\Assistant_fine_tunning_Spacy\dev.spacy"
vectors = "fr_core_news_md" # fr_core_news_md fournira les vecteurs initiaux
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = "fr"
pipeline = ["tok2vec", "ner"]
batch_size = 1000
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.tok2vec]
factory = "tok2vec"
# On retire la ligne 'source = "fr_core_news_md"' d'ici.
# L'initialisation se fera via [initialize.vectors] et l'architecture définie ci-dessous.
[components.tok2vec.model] # On définit explicitement l'architecture ici
@architectures = "spacy.HashEmbedCNN.v2" 
# pretrained_vectors = ${paths.vectors} # Optionnel: pour charger explicitement les vecteurs ici
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true
include_static_vectors = true # Mettre à true pour utiliser les vecteurs de paths.vectors

[components.ner]
factory = "ner"
source = "fr_core_news_md" # On garde 'source' ici car on veut fine-tuner le NER de fr_core_news_md

[components.ner.model] # Cette architecture sera celle du NER de fr_core_news_md
@architectures = "spacy.TransitionBasedParser.v2"
state_type = "ner"
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = true

[components.ner.model.tok2vec]
@architectures = "spacy.Tok2VecListener.v1"
width = ${components.tok2vec.model.encode.width} # Référence à la largeur du tok2vec principal
upstream = "*"

# ... (Le reste de la config : [corpora], [training], [initialize] reste identique
#      à la version qui a corrigé le problème du batcher) ...

[corpora]
[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
max_length = 0

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
max_length = 0

[training]
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 1600
max_epochs = 0
max_steps = 5000 
eval_frequency = 200
frozen_components = []
score_weights = {"ents_f": 1.0}

[training.optimizer]
@optimizers = "Adam.v1"
learn_rate = 0.001 # N'oubliez pas de remettre les autres paramètres d'optimizer si besoin

[training.batcher]
@batchers = "spacy.batch_by_words.v1"
discard_oversize = false
tolerance = 0.2

[training.batcher.size]
@schedules = "compounding.v1"
start = 100
stop = 1000
compound = 1.001
[initialize]
vectors = ${paths.vectors}
    